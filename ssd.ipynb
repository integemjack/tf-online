{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "788n7BqZKMda",
    "outputId": "54c6373b-6a1c-49cd-de3e-7ebd5e01516e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cu117\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yusLHocNL8pM",
    "outputId": "110b1458-8796-47c7-9ede-e44bad91fa10",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'pytorch-ssd'...\n",
      "remote: Enumerating objects: 1004, done.\u001b[K\n",
      "remote: Counting objects: 100% (85/85), done.\u001b[K\n",
      "remote: Compressing objects: 100% (48/48), done.\u001b[K\n",
      "remote: Total 1004 (delta 49), reused 62 (delta 36), pack-reused 919\u001b[K\n",
      "Receiving objects: 100% (1004/1004), 1.11 MiB | 86.00 KiB/s, done.\n",
      "Resolving deltas: 100% (655/655), done.\n",
      "/home/pytorch-ssd\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/dusty-nv/pytorch-ssd.git\n",
    "%cd pytorch-ssd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qLalhTqiMLpS",
    "outputId": "aa058385-fd65-4b50-e3d6-e7ecc33952de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LICENSE    eval_ssd.py\t   open_images_classes.txt    run_ssd_example.py\n",
      "README.md  models\t   open_images_downloader.py  train_ssd.py\n",
      "data\t   onnx_export.py  requirements.txt\t      vision\n"
     ]
    }
   ],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Use demo training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HoFCosjUMVZO",
    "outputId": "2df0f370-450f-4533-b3bd-a4382b132f19"
   },
   "source": [
    "! python3 open_images_downloader.py --class-names \"Apple\" --data=data/fruit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_VIL4-ggNVYg",
    "outputId": "4d3d52f5-4041-4c0a-83c6-27c8cff1829f"
   },
   "source": [
    "! python3 train_ssd.py --data=data/fruit --model-dir=models/fruit --batch-size=4 --epochs=30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you have an nvidia graphics card on your computer and installed the latest driver, you can use this method to directly export the onnx file, or send it back to the nano to produce an onnx file\n",
    "! python3 onnx_export.py --model-dir=models/fruit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Use custom training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# jetson nano IP address\n",
    "os.environ[\"NANO\"] = \"192.168.2.10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! echo \"scp -r nvidia@$NANO:/home/nvidia/jetson-inference/python/training/detection/ssd/data/custom ./data/custom\" | bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 train_ssd.py --dataset-type=voc --data=data/custom --model-dir=models/custom --workers=1 --batch-size=4 --epochs=30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## If you have an nvidia graphics card on your computer and installed the latest driver, you can use this method to directly export the onnx file, or send it back to the nano to produce an onnx file\n",
    "! python3 onnx_export.py --model-dir=models/custom\n",
    "! echo \"scp -r ./models/custom/xxx.onnx nvidia@$NANO:/home/nvidia/mqtt_ws/src/mqtt/models/\" | bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! echo \"scp -r ./models/custom/xxx.pth nvidia@$NANO:/home/nvidia/jetson-inference/python/training/detection/ssd/models/custom/\" | bash"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
