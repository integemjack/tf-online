{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "788n7BqZKMda",
    "outputId": "54c6373b-6a1c-49cd-de3e-7ebd5e01516e"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n"
     ]
    }
   ],
   "source": [
    "%cd /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yusLHocNL8pM",
    "outputId": "110b1458-8796-47c7-9ede-e44bad91fa10",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'jetson-inference'...\n",
      "remote: Enumerating objects: 27212, done.\u001b[K\n",
      "remote: Counting objects: 100% (5491/5491), done.\u001b[K\n",
      "remote: Compressing objects: 100% (1458/1458), done.\u001b[K\n",
      "remote: Total 27212 (delta 4078), reused 5157 (delta 3959), pack-reused 21721\u001b[K\n",
      "Receiving objects: 100% (27212/27212), 152.69 MiB | 2.54 MiB/s, done.\n",
      "Resolving deltas: 100% (22001/22001), done.\n",
      "Updating files: 100% (2223/2223), done.\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/dusty-nv/jetson-inference.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/jetson-inference\n"
     ]
    }
   ],
   "source": [
    "%cd jetson-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submodule 'plugins/pose' (https://github.com/dusty-nv/trt_pose) registered for path 'c/plugins/pose'\n",
      "Submodule 'docker/containers' (https://github.com/dusty-nv/jetson-containers) registered for path 'docker/containers'\n",
      "Submodule 'python/training/classification' (https://github.com/dusty-nv/pytorch-classification) registered for path 'python/training/classification'\n",
      "Submodule 'python/training/detection' (https://github.com/dusty-nv/pytorch-detection) registered for path 'python/training/detection'\n",
      "Submodule 'python/training/segmentation' (https://github.com/dusty-nv/pytorch-segmentation) registered for path 'python/training/segmentation'\n",
      "Submodule 'ros' (https://github.com/dusty-nv/ros_deep_learning) registered for path 'ros'\n",
      "Submodule 'tools/camera-capture' (https://github.com/dusty-nv/camera-capture) registered for path 'tools/camera-capture'\n",
      "Submodule 'utils' (https://github.com/dusty-nv/jetson-utils) registered for path 'utils'\n",
      "Cloning into '/jetson-inference/c/plugins/pose'...\n",
      "Cloning into '/jetson-inference/docker/containers'...\n",
      "Cloning into '/jetson-inference/python/training/classification'...\n",
      "Cloning into '/jetson-inference/python/training/detection'...\n",
      "Cloning into '/jetson-inference/python/training/segmentation'...\n",
      "Cloning into '/jetson-inference/ros'...\n"
     ]
    }
   ],
   "source": [
    "! git submodule update --init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BKE3apS_MDXA",
    "outputId": "1a092f94-2b1f-41c3-9d4b-9144252ce229"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification\tdetection  segmentation\n",
      "[Errno 2] No such file or directory: '/jetson-inference/python/training/detection/ssd'\n",
      "/jetson-inference/python/training\n"
     ]
    }
   ],
   "source": [
    "%cd python/training/detection/ssd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qLalhTqiMLpS",
    "outputId": "aa058385-fd65-4b50-e3d6-e7ecc33952de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification\tdetection  segmentation\n"
     ]
    }
   ],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use demo training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HoFCosjUMVZO",
    "outputId": "2df0f370-450f-4533-b3bd-a4382b132f19"
   },
   "outputs": [],
   "source": [
    "! python3 open_images_downloader.py --class-names \"Apple\" --data=data/fruit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_VIL4-ggNVYg",
    "outputId": "4d3d52f5-4041-4c0a-83c6-27c8cff1829f"
   },
   "outputs": [],
   "source": [
    "! python3 train_ssd.py --data=data/fruit --model-dir=models/fruit --batch-size=4 --epochs=30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you have an nvidia graphics card on your computer and installed the latest driver, you can use this method to directly export the onnx file, or send it back to the nano to produce an onnx file\n",
    "! python3 onnx_export.py --model-dir=models/fruit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use custom training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# jetson nano IP address\n",
    "os.environ[\"NANO\"] = \"192.168.2.10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kex_exchange_identification: Connection closed by remote host\n"
     ]
    }
   ],
   "source": [
    "! echo \"scp -r nvidia:cc880108@$NANO:/home/jetson-inference/python/training/detection/ssd/data/custom ./data/custom\" | bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: train_ssd.py [-h] [--dataset-type DATASET_TYPE]\n",
      "                    [--datasets DATASETS [DATASETS ...]] [--balance-data]\n",
      "                    [--net NET] [--resolution RESOLUTION] [--freeze-base-net]\n",
      "                    [--freeze-net] [--mb2-width-mult MB2_WIDTH_MULT]\n",
      "                    [--base-net BASE_NET] [--pretrained-ssd PRETRAINED_SSD]\n",
      "                    [--resume RESUME] [--lr LR] [--momentum MOMENTUM]\n",
      "                    [--weight-decay WEIGHT_DECAY] [--gamma GAMMA]\n",
      "                    [--base-net-lr BASE_NET_LR]\n",
      "                    [--extra-layers-lr EXTRA_LAYERS_LR]\n",
      "                    [--scheduler SCHEDULER] [--milestones MILESTONES]\n",
      "                    [--t-max T_MAX] [--batch-size BATCH_SIZE]\n",
      "                    [--num-epochs NUM_EPOCHS] [--num-workers NUM_WORKERS]\n",
      "                    [--validation-epochs VALIDATION_EPOCHS]\n",
      "                    [--validation-mean-ap] [--debug-steps DEBUG_STEPS]\n",
      "                    [--use-cuda] [--checkpoint-folder CHECKPOINT_FOLDER]\n",
      "                    [--log-level LOG_LEVEL]\n",
      "train_ssd.py: error: unrecognized arguments: --dataset-tpye=voc --worders=1\n"
     ]
    }
   ],
   "source": [
    "! python3 train_ssd.py --dataset-tpye=voc --data=data/custom --model-dir=models/custom --worders=1 --batch-size=4 --epochs=30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you have an nvidia graphics card on your computer and installed the latest driver, you can use this method to directly export the onnx file, or send it back to the nano to produce an onnx file\n",
    "! python3 onnx_export.py --model-dir=models/custom\n",
    "! echo \"scp -r ./models/custom/xxx.onnx nvidia@$NANO:/home/nvidia/mqtt_ws/src/mqtt/models/\" | bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! echo \"scp -r ./models/custom/xxx.pth nvidia@$NANO:/home/jetson-inference/python/training/detection/ssd/models/custom/\" | bash"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
